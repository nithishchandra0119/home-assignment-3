Q1: Implementing a Basic Autoencoder
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
# 1. Load the MNIST dataset
(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()
# Normalize and flatten the images
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))
# 2. Define the autoencoder
input_dim = 784
latent_dim = 32
input_layer = tf.keras.layers.Input(shape=(input_dim,))
encoder = tf.keras.layers.Dense(latent_dim, activation='relu')(input_layer)
decoder = tf.keras.layers.Dense(input_dim, activation='sigmoid')(encoder)
autoencoder = tf.keras.models.Model(input_layer, decoder)
# 3. Compile and train the autoencoder
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
autoencoder.fit(x_train, x_train,
epochs=50,
batch_size=256,
shuffle=True,
validation_data=(x_test, x_test))
# 4. Plot original vs. reconstructed images
decoded_imgs = autoencoder.predict(x_test)
n = 10
plt.figure(figsize=(20, 4))
for i in range(n):
# Display original
ax = plt.subplot(2, n, i + 1)
plt.imshow(x_test[i].reshape(28, 28))
plt.gray()
ax.get_xaxis().set_visible(False)
ax.get_yaxis().set_visible(False)
# Display reconstruction
ax = plt.subplot(2, n, i + 1 + n)
plt.imshow(decoded_imgs[i].reshape(28, 28))
plt.gray()
ax.get_xaxis().set_visible(False)
ax.get_yaxis().set_visible(False)
plt.show()
# 5. Modify latent dimension and analyze
latent_dims = [16, 64]
for latent_dim in latent_dims:
input_layer = tf.keras.layers.Input(shape=(input_dim,))
encoder = tf.keras.layers.Dense(latent_dim, activation='relu')(input_layer)
decoder = tf.keras.layers.Dense(input_dim, activation='sigmoid')(encoder)
autoencoder = tf.keras.models.Model(input_layer, decoder)
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
autoencoder.fit(x_train, x_train,
epochs=50,
batch_size=256,
shuffle=True,
validation_data=(x_test, x_test),
verbose=0)
decoded_imgs = autoencoder.predict(x_test)
print(f"Latent Dimension: {latent_dim}")
n = 5
plt.figure(figsize=(20, 4))
for i in range(n):
# Display original
ax = plt.subplot(2, n, i + 1)
plt.imshow(x_test[i].reshape(28, 28))
plt.gray()
ax.get_xaxis().set_visible(False)
ax.get_yaxis().set_visible(False)
# Display reconstruction
ax = plt.subplot(2, n, i + 1 + n)
plt.imshow(decoded_imgs[i].reshape(28, 28))
plt.gray()
ax.get_xaxis().set_visible(False)
ax.get_yaxis().set_visible(False)
plt.show()


Q2: Implementing a Denoising Autoencoder
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
# Load the MNIST dataset
(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()
# Normalize and reshape the data
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
x_train = x_train.reshape((len(x_train), 28, 28, 1))
x_test = x_test.reshape((len(x_test), 28, 28, 1))
# Add Gaussian noise to the input images
noise_factor = 0.5
x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)
x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)
x_train_noisy = np.clip(x_train_noisy, 0., 1.)
x_test_noisy = np.clip(x_test_noisy, 0., 1.)
# Define the denoising autoencoder model
input_img = tf.keras.layers.Input(shape=(28, 28, 1))
# Encoder
x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)
x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)
x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)
encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)
# Decoder
x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)
x = tf.keras.layers.UpSampling2D((2, 2))(x)
x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)
x = tf.keras.layers.UpSampling2D((2, 2))(x)
decoded = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)
autoencoder = tf.keras.models.Model(input_img, decoded)
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
# Train the denoising autoencoder
autoencoder.fit(x_train_noisy, x_train, epochs=50, batch_size=128, shuffle=True,
validation_data=(x_test_noisy, x_test))
# Predict the reconstructed images
decoded_imgs = autoencoder.predict(x_test_noisy)
# Visualize noisy vs. reconstructed images
n = 10
plt.figure(figsize=(20, 4))
for i in range(n):
# Display original
ax = plt.subplot(2, n, i + 1)
plt.imshow(x_test_noisy[i].reshape(28, 28))
plt.gray()
ax.get_xaxis().set_visible(False)
ax.get_yaxis().set_visible(False)
# Display reconstruction
ax = plt.subplot(2, n, i + 1 + n)
plt.imshow(decoded_imgs[i].reshape(28, 28))
plt.gray()
ax.get_xaxis().set_visible(False)
ax.get_yaxis().set_visible(False)
plt.show()
# Basic autoencoder for comparison
input_img_basic = tf.keras.layers.Input(shape=(28, 28, 1))
x_basic = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img_basic)
x_basic = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x_basic)
x_basic = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x_basic)
encoded_basic = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x_basic)
x_basic = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded_basic)
x_basic = tf.keras.layers.UpSampling2D((2, 2))(x_basic)
x_basic = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x_basic)
x_basic = tf.keras.layers.UpSampling2D((2, 2))(x_basic)
decoded_basic = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x_basic)
basic_autoencoder = tf.keras.models.Model(input_img_basic, decoded_basic)
basic_autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
basic_autoencoder.fit(x_train, x_train, epochs=50, batch_size=128, shuffle=True, validation_data=(x_test,
x_test))
decoded_basic_imgs = basic_autoencoder.predict(x_test)
plt.figure(figsize=(20, 4))
for i in range(n):
ax = plt.subplot(2, n, i + 1)
plt.imshow(x_test[i].reshape(28, 28))
plt.gray()
ax.get_xaxis().set_visible(False)
ax.get_yaxis().set_visible(False)
ax = plt.subplot(2, n, i + 1 + n)
plt.imshow(decoded_basic_imgs[i].reshape(28, 28))
plt.gray()
ax.get_xaxis().set_visible(False)
ax.get_yaxis().set_visible(False)
plt.show()
# Compare performance
print("Denoising Autoencoder Results")
mse_denoising = np.mean((x_test - decoded_imgs)**2)
print(f"MSE Denoising: {mse_denoising}")
print("Basic Autoencoder Results")
mse_basic = np.mean((x_test - decoded_basic_imgs)**2)
print(f"MSE Basic: {mse_basic}")
# Real-world scenario: Medical Imaging
print("\nReal-world scenario: Medical Imaging")
print("Denoising autoencoders can be used to improve the quality of medical images, such as MRI or CT
scans. These images are often corrupted by noise due to various factors, such as low signal strength or
patient movement. By training a denoising autoencoder on a dataset of clean medical images, it can learn
to remove noise from new, noisy images, leading to better diagnostic accuracy.")



Q3: Implementing an RNN for Text Generation
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam
# 1. Load and preprocess the text dataset
text_path = "shakespeare_sonnets.txt" # Replace with the actual file path
try:
with open(text_path, 'r') as file:
text = file.read().lower()
except FileNotFoundError:
print(f"File not found: {text_path}. Upload the file or update the path.")
# Create character mappings
chars = sorted(set(text))
char_to_index = {char: idx for idx, char in enumerate(chars)}
index_to_char = {idx: char for idx, char in enumerate(chars)}
# Encode text and create sequences
sequence_length = 100
encoded_text = [char_to_index[char] for char in text]
sequences = []
next_chars = []
for i in range(0, len(encoded_text) - sequence_length):
sequences.append(encoded_text[i:i+sequence_length])
next_chars.append(encoded_text[i+sequence_length])
# Convert sequences to one-hot encoding
num_features = len(chars)
X = tf.keras.utils.to_categorical(sequences, num_classes=num_features)
y = tf.keras.utils.to_categorical(next_chars, num_classes=num_features)
# 2. Define the RNN model
model = Sequential([
LSTM(256, input_shape=(sequence_length, num_features)),
Dense(num_features, activation='softmax')
])
model.compile(loss="categorical_crossentropy", optimizer=Adam(learning_rate=0.001))
# 3. Train the model
model.fit(X, y, epochs=20, batch_size=128)
# 4. Generate new text
def sample(predictions, temperature=1.0):
predictions = np.log(predictions + 1e-8) / temperature
exp_preds = np.exp(predictions)
probabilities = exp_preds / np.sum(exp_preds)
return np.random.choice(len(probabilities), p=probabilities)
seed_text = "to be or not to be "
generated_text = seed_text
for _ in range(200):
seed_encoded = [char_to_index[char] for char in seed_text[-sequence_length:]]
seed_one_hot = tf.keras.utils.to_categorical(seed_encoded, num_classes=num_features).reshape(1,
sequence_length, num_features)
predictions = model.predict(seed_one_hot)[0]
next_char_idx = sample(predictions, temperature=0.8)
next_char = index_to_char[next_char_idx]
generated_text += next_char
print("Generated Text:\n", generated_text)
# 5. Explain the role of temperature scaling
print("\n5. Explain the role of temperature scaling in text generation and its effect on randomness.")
print("Temperature scaling in text generation controls the randomness of the output. Higher temperatures
(e.g., 2.0) make the output more random by giving more weight to less likely characters, leading to more
diverse but potentially nonsensical text. Lower temperatures (e.g., 0.5) make the output more
deterministic by favoring the most likely characters, resulting in more predictable and conservative text.
A temperature of 1.0 represents the default level of randomness. Essentially, the temperature parameter
scales the logits before the softmax function, altering the probability distribution from which the next
character is sampled.")



Q4: Sentiment Classification Using RNN
import tensorflow as tf
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from sklearn.metrics import confusion_matrix, classification_report
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# 1. Load the IMDB sentiment dataset
max_features = 10000 # Number of words to consider as features
maxlen = 500 # Maximum sequence length
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=max_features)
# 2. Preprocess the text data by tokenization and padding sequences
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
# 3. Train an LSTM-based model to classify reviews as positive or negative
model = Sequential()
model.add(Embedding(max_features, 128))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
batch_size = 32
epochs = 5
history = model.fit(x_train, y_train,
batch_size=batch_size,
epochs=epochs,
validation_split=0.2)
# 4. Generate a confusion matrix and classification report
y_pred = model.predict(x_test)
y_pred_binary = (y_pred > 0.5).astype(int)
cm = confusion_matrix(y_test, y_pred_binary)
cr = classification_report(y_test, y_pred_binary)
print("Confusion Matrix:")
print(cm)
print("\nClassification Report:")
print(cr)
# Plotting Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()
# Plotting Accuracy and Loss
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Training and Validation Accuracy')
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Training and Validation Loss')




